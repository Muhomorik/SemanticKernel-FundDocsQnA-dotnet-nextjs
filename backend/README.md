# Backend API - PDF Q&A Application

ASP.NET Core Web API backend that provides semantic search and question answering capabilities for PDF documents using Semantic Kernel, Ollama embeddings, and Groq LLM.

## Overview

This backend API loads pre-generated embeddings from the Preprocessor, stores them in an in-memory vector store, and provides endpoints to search and answer questions about your PDF documents.

## Architecture

```plaintext
Request Flow:
1. POST /api/ask with question
2. Generate embedding for question (Ollama)
3. Search for similar chunks in memory store
4. Build context from top K results
5. Send context + question to Groq LLM
6. Return answer + source references
```

## Prerequisites

### Required

- **.NET 9 SDK** - Download from [dotnet.microsoft.com](https://dotnet.microsoft.com/download)
- **Ollama** - For generating query embeddings (same model as Preprocessor)
  - Download from [ollama.com](https://ollama.com)
  - Pull the embedding model: `ollama pull nomic-embed-text`
- **Groq API Key** - Free tier available
  - Sign up at [console.groq.com](https://console.groq.com)
  - Create an API key in the dashboard

### Optional

- **embeddings.json** - Generated by the Preprocessor
  - Copy from Preprocessor output to `backend/Backend.API/Data/embeddings.json`

## Setup Instructions

### 1. Clone and Navigate

```bash
cd backend
```

### 2. Configure Settings

#### Option A: appsettings.Development.json (Recommended for Development)

Edit `Backend.API/appsettings.Development.json`:

```json
{
  "BackendOptions": {
    "GroqApiKey": "your-groq-api-key-here"
  }
}
```

#### Option B: Environment Variables (Recommended for Production)

```bash
# Windows (PowerShell)
$env:GROQ_API_KEY="your-groq-api-key-here"
$env:EMBEDDINGS_PATH="path/to/your/embeddings.json"

# Linux/macOS
export GROQ_API_KEY="your-groq-api-key-here"
export EMBEDDINGS_PATH="path/to/your/embeddings.json"
```

### 3. Copy Embeddings File

```bash
# Copy from Preprocessor output
cp ../Preprocessor/Preprocessor/bin/Debug/net9.0/output.json Backend.API/Data/embeddings.json
```

### 4. Start Ollama

```bash
# Start Ollama server
ollama serve

# In another terminal, verify the model is available
ollama pull nomic-embed-text
```

### 5. Run the Backend

```bash
cd Backend.API
dotnet run
```

The API will start at:

- HTTP: `http://localhost:5000`
- HTTPS: `https://localhost:5001`
- Swagger UI: `http://localhost:5000/swagger`

## Configuration Options

All configuration is in `appsettings.json` under the `BackendOptions` section:

| Setting | Default | Description |
|---------|---------|-------------|
| `EmbeddingsFilePath` | `Data/embeddings.json` | Path to embeddings JSON file |
| `GroqApiKey` | `""` | Groq API key (override via env var) |
| `GroqModel` | `llama-3.3-70b-versatile` | Groq LLM model to use |
| `GroqApiUrl` | `https://api.groq.com/openai/v1` | Groq API endpoint |
| `EmbeddingModel` | `nomic-embed-text` | Ollama embedding model |
| `OllamaUrl` | `http://localhost:11434` | Ollama server URL |
| `MaxSearchResults` | `5` | Number of chunks to retrieve |
| `MemoryCollectionName` | `fund-documents` | Memory store collection name |

## API Endpoints

### POST /api/ask

Ask a question about your PDF documents.

**Request:**

```bash
curl -X POST http://localhost:5000/api/ask \
  -H "Content-Type: application/json" \
  -d '{"question":"What is the risk level of Spiltan Globalfond?"}'
```

**Request Body:**

```json
{
  "question": "What is the risk level of Spiltan Globalfond?"
}
```

**Response:**

```json
{
  "answer": "The fund is classified as risk level 4 out of 7, indicating moderate risk.",
  "sources": [
    {
      "file": "Spiltan_Globalfond.pdf",
      "page": 1
    }
  ]
}
```

**Status Codes:**

- `200 OK` - Success
- `400 Bad Request` - Invalid question (empty or too short)
- `500 Internal Server Error` - Processing error

### GET /api/health

Check the health and status of the backend.

**Request:**

```bash
curl http://localhost:5000/api/health
```

**Response:**

```json
{
  "status": "Healthy",
  "embeddingsLoaded": true,
  "embeddingCount": 42
}
```

**Status Codes:**

- `200 OK` - Always returns 200

## Running in Production

### 1. Set Environment Variables

```bash
export GROQ_API_KEY="your-production-groq-api-key"
export EMBEDDINGS_PATH="/app/data/embeddings.json"
export ASPNETCORE_ENVIRONMENT="Production"
```

### 2. Build and Run

```bash
dotnet publish -c Release -o ./publish
cd publish
dotnet Backend.API.dll
```

### 3. Deploy

The backend can be deployed to:

- **Azure App Service**
- **Docker** (create a Dockerfile)

## Testing

### Manual Testing

1. **Start all dependencies:**

```bash
# Terminal 1: Start Ollama
ollama serve

# Terminal 2: Start Backend
cd backend/Backend.API
dotnet run
```

1. **Test health endpoint:**

```bash
curl http://localhost:5000/api/health
```

Expected: `{"status":"Healthy","embeddingsLoaded":true,"embeddingCount":...}`

1. **Test ask endpoint:**

```bash
curl -X POST http://localhost:5000/api/ask \
  -H "Content-Type: application/json" \
  -d '{"question":"What are the key characteristics of the funds?"}'
```

### Run Unit Tests

```bash
cd Backend.Tests
dotnet test
```

## Troubleshooting

### Issue: "Failed to initialize memory service"

**Causes:**

1. Embeddings file doesn't exist
2. Ollama is not running
3. Embedding model not available in Ollama

**Solutions:**

```bash
# Check embeddings file exists
ls Backend.API/Data/embeddings.json

# Start Ollama
ollama serve

# Pull embedding model
ollama pull nomic-embed-text

# Verify Ollama is running
curl http://localhost:11434/api/tags
```

### Issue: "Groq API key is not set"

**Solution:**

```bash
# Set environment variable
export GROQ_API_KEY="your-key-here"

# Or update appsettings.Development.json
```

### Issue: "Connection refused" when calling Ollama

**Solution:**

```bash
# Check if Ollama is running
curl http://localhost:11434/api/tags

# If not running, start it
ollama serve
```

### Issue: "Model 'nomic-embed-text' not found"

**Solution:**

```bash
# Pull the model
ollama pull nomic-embed-text

# Verify it's available
ollama list
```

### Issue: CORS errors from frontend

**Solution:**

Update `Program.cs` to include your frontend URL:

```csharp
policy.WithOrigins("http://localhost:3000", "https://your-frontend-url.com")
```

## Development

### Project Structure

```plaintext
Backend.API/
├── Configuration/
│   └── BackendOptions.cs          # Configuration model
├── Controllers/
│   ├── AskController.cs            # POST /api/ask
│   └── HealthController.cs         # GET /api/health
├── Models/
│   ├── EmbeddingRecord.cs          # Matches Preprocessor output
│   ├── AskRequest.cs               # API request models
│   ├── AskResponse.cs
│   ├── SourceReference.cs
│   └── HealthResponse.cs
├── Services/
│   ├── IMemoryService.cs
│   ├── MemoryService.cs            # Loads and searches embeddings
│   ├── IQuestionAnsweringService.cs
│   └── QuestionAnsweringService.cs # Orchestrates Q&A flow
├── Data/
│   └── embeddings.json             # Copy from Preprocessor
├── Program.cs                      # App startup and DI
└── appsettings.json                # Configuration
```

### Adding More Documents

When you add new PDFs using the Preprocessor:

1. Run Preprocessor with `--append` flag
2. Copy updated `embeddings.json` to backend
3. Restart the backend

```bash
# 1. In Preprocessor
cd ../Preprocessor/Preprocessor
dotnet run -- -m pdfpig -i ./pdfs -o ./bin/Debug/net9.0/output.json --append

# 2. Copy to backend
cp bin/Debug/net9.0/output.json ../../backend/Backend.API/Data/embeddings.json

# 3. Restart backend
cd ../../backend/Backend.API
dotnet run
```

## Tech Stack

- **ASP.NET Core 9** - Web API framework
- **Semantic Kernel 1.68.0** - AI orchestration
- **Ollama** - Local embeddings (nomic-embed-text)
- **Groq** - Cloud LLM (llama-3.3-70b-versatile)
- **VolatileMemoryStore** - In-memory vector store

## Next Steps

1. Create the Next.js frontend
2. Integrate frontend with this backend
3. Deploy to production
4. Add authentication (optional)
5. Implement caching (optional)

## License

MIT
