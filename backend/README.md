# Backend API - PDF Q&A Application

> Part of [PDF Q&A Application](../README.md). See [Configuration & Secrets Guide](../docs/SECRETS-MANAGEMENT.md) for all environment variables and API key setup.

ASP.NET Core Web API backend built with **Domain-Driven Design (DDD)** architecture that provides semantic search and question answering capabilities for PDF documents using Semantic Kernel, OpenAI embeddings, and configurable LLM provider (OpenAI or Groq).

## Overview

This backend API loads pre-generated embeddings from the Preprocessor, stores them in a vector store (in-memory or Azure Cosmos DB), and provides endpoints to search and answer questions about your PDF documents. The codebase follows DDD principles with clean separation of concerns across Domain, Application, Infrastructure, and Presentation layers.

**Vector Storage Options:**

- **InMemory (default)** - Embeddings loaded from `Data/embeddings.json` file. Fast, simple, suitable for development and small deployments. Embeddings are lost on app restart.
- **CosmosDb (optional)** - Persistent vector storage with Azure Cosmos DB. Enables dynamic updates, multi-instance deployments, and scales to production workloads. Requires Azure Cosmos DB account.

## Tech Stack

- **ASP.NET Core 9** - Web API framework
- **Semantic Kernel 1.68.0** - AI orchestration with InMemoryVectorStore
- **OpenAI** - Query embeddings (text-embedding-3-small) + Chat completion (gpt-4o-mini, default)
- **Groq** - Optional cloud LLM (llama-3.3-70b-versatile, free tier)
- **Azure Application Insights** - Monitoring and telemetry (free tier)
- **Azure Key Vault** - Secrets management (production)

## Prerequisites

- **.NET 9 SDK** - Download from [dotnet.microsoft.com](https://dotnet.microsoft.com/download)
- **API Keys** - OpenAI (required), Groq (optional) - See [Configuration & Secrets Guide](../docs/SECRETS-MANAGEMENT.md)
- **embeddings.json** - Generated by the Preprocessor (copy to `Backend.API/Data/embeddings.json`)

## Architecture

### Request Flow (RAG Pipeline)

```plaintext
1. POST /api/ask with question
2. Generate embedding for question (OpenAI text-embedding-3-small)
3. Semantic search via InMemoryVectorStore (built-in cosine similarity)
4. Build context from top K results
5. Send context + question to LLM (OpenAI gpt-4o-mini or Groq llama-3.3-70b-versatile)
6. Return answer + source references
```

```plaintext
## Setup Instructions

### 1. Clone and Navigate

```bash
cd backend
```

### 2. Configure API Keys

Set API keys using .NET User Secrets (recommended for local development):

**Option 1: Use OpenAI (default, recommended):**

```bash
cd Backend.API
dotnet user-secrets set "BackendOptions:OpenAIApiKey" "sk-your-openai-api-key"
dotnet user-secrets set "BackendOptions:LlmProvider" "OpenAI"
```

**Option 2: Use Groq (free tier alternative):**

```bash
cd Backend.API
dotnet user-secrets set "BackendOptions:OpenAIApiKey" "sk-your-openai-api-key"  # Still needed for embeddings
dotnet user-secrets set "BackendOptions:GroqApiKey" "gsk-your-groq-api-key"
dotnet user-secrets set "BackendOptions:LlmProvider" "Groq"
```

See **[Configuration & Secrets Guide](../docs/SECRETS-MANAGEMENT.md)** for all configuration options, environment variables, and production setup.

### 3. Copy Embeddings File

**Important:** Ensure embeddings were generated using OpenAI (not Ollama) for vector space compatibility.

```bash
# Copy from Preprocessor output
cp ../Preprocessor/Preprocessor/bin/Debug/net9.0/output.json Backend.API/Data/embeddings.json
```

### 4. Run the Backend

```bash
cd Backend.API
dotnet run
```

The API will start at:

- HTTP: `http://localhost:5000`
- HTTPS: `https://localhost:5001`
- Swagger UI: `http://localhost:5000/swagger`
- Health (Live): `http://localhost:5000/health/live`
- Health (Ready): `http://localhost:5000/health/ready`

## Configuration Options

All configuration is in `appsettings.json` under the `BackendOptions` section. See **[Configuration & Secrets Guide](../docs/SECRETS-MANAGEMENT.md)** for complete details.

| Setting                   | Default                       | Description                                                      |
| ------------------------- | ----------------------------- | ---------------------------------------------------------------- |
| `VectorStorageType`       | `InMemory`                    | Vector storage backend ("InMemory" or "CosmosDb")                |
| `EmbeddingsFilePath`      | `Data/embeddings.json`        | Path to embeddings JSON file (InMemory only)                     |
| `CosmosDbEndpoint`        | (none)                        | Cosmos DB account endpoint (CosmosDb only)                       |
| `CosmosDbDatabaseName`    | (none)                        | Cosmos DB database name (CosmosDb only)                          |
| `CosmosDbContainerName`   | `embeddings`                  | Cosmos DB container name (CosmosDb only)                         |
| `EmbeddingApiKey`         | (none)                        | API key for embedding endpoints (CosmosDb only)                  |
| `LlmProvider`             | `OpenAI`                      | LLM provider ("OpenAI" or "Groq")                                |
| `OpenAIChatModel`         | `gpt-4o-mini`                 | OpenAI chat model (when LlmProvider is "OpenAI")                 |
| `OpenAIEmbeddingModel`    | `text-embedding-3-small`      | OpenAI embedding model                                           |
| `GroqModel`               | `llama-3.3-70b-versatile`     | Groq LLM model (when LlmProvider is "Groq")                      |
| `MaxSearchResults`        | `10`                          | Number of chunks to retrieve                                     |
| `SystemPrompt`            | (See ApplicationOptions)      | Custom LLM system prompt (optional, uses default if not provided)|

## Switching LLM Providers

The backend supports two LLM providers for chat completion:

### OpenAI (Default, Recommended)

**Advantages:**

- Higher quality responses
- More reliable availability
- Broader model selection
- Official API support

**Cost:** ~$0.15 per 1M input tokens, ~$0.60 per 1M output tokens (gpt-4o-mini)

**Setup:**

```bash
cd Backend.API
dotnet user-secrets set "BackendOptions:LlmProvider" "OpenAI"
dotnet user-secrets set "BackendOptions:OpenAIApiKey" "sk-your-openai-api-key"
dotnet user-secrets set "BackendOptions:OpenAIChatModel" "gpt-4o-mini"
```

### Groq (Free Tier Alternative)

**Advantages:**

- Zero cost (free tier)
- Fast inference
- OpenAI-compatible API

**Limitations:**

- Rate limits on free tier
- Limited model selection
- Third-party service

**Setup:**

```bash
cd Backend.API
dotnet user-secrets set "BackendOptions:LlmProvider" "Groq"
dotnet user-secrets set "BackendOptions:OpenAIApiKey" "sk-your-openai-api-key"  # Still needed for embeddings
dotnet user-secrets set "BackendOptions:GroqApiKey" "gsk-your-groq-api-key"
```

## Switching Vector Storage

The backend supports two vector storage backends:

### InMemory (Default)

**Advantages:**

- Simple setup (no external dependencies)
- Fast search performance (in-memory)
- Zero infrastructure cost
- Ideal for development and demos

**Limitations:**

- Embeddings lost on app restart
- Cannot update embeddings without redeployment
- Single-instance only (no shared state)
- Limited by server memory

**Setup:**

```bash
cd Backend.API

# Use InMemory storage (default)
dotnet user-secrets set "BackendOptions:VectorStorageType" "InMemory"

# Ensure embeddings.json exists
cp ../../Preprocessor/Preprocessor/bin/Debug/net9.0/output.json Data/embeddings.json
```

**How it works:**

1. Backend loads `Data/embeddings.json` on startup
2. Embeddings stored in memory using Semantic Kernel InMemoryVectorStore
3. Search uses cosine similarity
4. Embeddings persist until app restart

### Cosmos DB (Production, Optional)

**Advantages:**

- Persistent storage (embeddings survive restarts)
- Dynamic updates via API (no redeployment needed)
- Multi-instance support (shared vector store)
- Scales to production workloads
- Native vector indexing for efficient search

**Limitations:**

- Requires Azure Cosmos DB account
- Additional infrastructure cost (free tier available)
- Slightly higher latency vs. in-memory

**Setup (Development):**

```bash
cd Backend.API

# Enable Cosmos DB storage
dotnet user-secrets set "BackendOptions:VectorStorageType" "CosmosDb"

# Set Cosmos DB endpoint
dotnet user-secrets set "BackendOptions:CosmosDbEndpoint" "https://<your-cosmos-account>.documents.azure.com:443/"

# Set database and container names
dotnet user-secrets set "BackendOptions:CosmosDbDatabaseName" "<your-database-name>"
dotnet user-secrets set "BackendOptions:CosmosDbContainerName" "embeddings"

# Set connection string (development only)
dotnet user-secrets set "BackendOptions:CosmosDbConnectionString" "AccountEndpoint=https://<your-cosmos-account>.documents.azure.com:443/;AccountKey=<your-account-key>;"

# Generate and set API key for Preprocessor authentication
dotnet user-secrets set "BackendOptions:EmbeddingApiKey" "$(openssl rand -base64 32)"
```

**Setup (Production - Managed Identity):**

See [AZURE-DEPLOYMENT.md](../docs/AZURE-DEPLOYMENT.md#optional-cosmos-db-setup-for-persistent-vector-storage) for complete production setup with Managed Identity authentication.

**Uploading Embeddings to Cosmos DB:**

Use the Preprocessor with `--cosmosdb` flag to upload embeddings to the Backend API:

```bash
cd ../../Preprocessor/Preprocessor

# Generate and upload embeddings to Cosmos DB
dotnet run -- \
  --provider openai \
  --input-dir ../../pdfs \
  --cosmosdb \
  --url "http://localhost:5000" \
  --api-key "<your-embedding-api-key>"
```

**API Endpoints for Embedding Management:**

When Cosmos DB storage is enabled, the following endpoints become available (protected by API key):

| Endpoint                          | Method | Description                              |
| --------------------------------- | ------ | ---------------------------------------- |
| `/api/embeddings`                 | POST   | Add new embeddings (batch)               |
| `/api/embeddings/{sourceFile}`    | PUT    | Update embeddings for specific file      |
| `/api/embeddings/{sourceFile}`    | DELETE | Delete embeddings for specific file      |
| `/api/embeddings/replace-all`     | POST   | Replace all embeddings (destructive)     |

**Authentication:**

All embedding endpoints require API key authentication:

```bash
curl -X POST http://localhost:5000/api/embeddings \
  -H "Content-Type: application/json" \
  -H "Authorization: ApiKey <your-embedding-api-key>" \
  -d '{"embeddings": [...]}'
```

**Health Check:**

When Cosmos DB storage is enabled, the `/health/ready` endpoint includes a Cosmos DB connectivity check:

```bash
curl http://localhost:5000/health/ready
```

Expected response:

```json
{
  "status": "Healthy",
  "results": {
    "memory_service": {
      "status": "Healthy",
      "description": "Document chunks loaded: 1234"
    },
    "cosmosdb": {
      "status": "Healthy",
      "description": "Cosmos DB connected: 1234 documents in '<database>/<container>'"
    }
  }
}
```

## API Endpoints

### POST /api/ask

Ask a question about your PDF documents.

**Request:**

```bash
curl -X POST http://localhost:5000/api/ask \
  -H "Content-Type: application/json" \
  -d '{"question":"What is the risk level of Spiltan Globalfond?"}'
```

**Request Body:**

```json
{
  "question": "What is the risk level of Spiltan Globalfond?"
}
```

**Response:**

```json
{
  "answer": "The fund is classified as risk level 4 out of 7, indicating moderate risk.",
  "sources": [
    {
      "file": "Spiltan_Globalfond.pdf",
      "page": 1
    }
  ]
}
```

**Status Codes:**

- `200 OK` - Success
- `400 Bad Request` - Invalid question (empty or too short)
- `500 Internal Server Error` - Processing error

### GET /health/live

Liveness probe - checks if the application is running.

**Request:**

```bash
curl http://localhost:5000/health/live
```

**Response:**

```plaintext
Healthy
```

**Status Codes:**

- `200 OK` - Application is running
- `503 Service Unavailable` - Application is not responding

### GET /health/ready

Readiness probe - checks if the application is ready to serve traffic.

**Request:**

```bash
curl http://localhost:5000/health/ready
```

**Response:**

```plaintext
Healthy
```

**Status Codes:**

- `200 OK` - Application is ready (embeddings loaded, dependencies available)
- `503 Service Unavailable` - Application is not ready

## Testing

### Manual Testing

1. **Start the backend:**

```bash
cd backend/Backend.API
dotnet run
```

1. **Test liveness endpoint:**

```bash
curl http://localhost:5000/health/live
```

Expected: `Healthy`

1. **Test readiness endpoint:**

```bash
curl http://localhost:5000/health/ready
```

Expected: `Healthy`

1. **Test ask endpoint:**

```bash
curl -X POST http://localhost:5000/api/ask \
  -H "Content-Type: application/json" \
  -d '{"question":"What are the key characteristics of the funds?"}'
```

Expected: JSON response with `answer` and `sources` fields

### Run Unit Tests

```bash
cd Backend.Tests
dotnet test
```

### Deployment Verification Tests

These tests verify the live Azure deployment is working correctly. They use NUnit's `[Explicit]` attribute, so they **won't run** with `dotnet test` or "Run All Tests" - you must select and run them manually.

**Setup (one-time):**

```bash
cd Backend.Tests
dotnet user-secrets set "DeploymentTests:AzureApiUrl" "https://replace-me-from-deployment-url"
```

**Running in Visual Studio:**

1. Open the solution in Visual Studio
2. Open Test Explorer (Test → Test Explorer)
3. Find "DeploymentVerificationTests" in the test list
4. Right-click → Run Selected Tests

**What gets tested:**

- `GET /health/live` - Liveness probe returns "Healthy"
- `GET /health/ready` - Readiness probe returns "Healthy"
- `POST /api/ask` - Valid question returns an answer
- `POST /api/ask` - Invalid question returns 400 Bad Request

**Note:** Azure App Service free tier has cold start delays (up to 30+ seconds) when the app is idle. The first test may take longer due to this warm-up time.

**Important:** Azure free tier (F1) has daily quotas (60 CPU minutes, 1 GB memory). When exceeded, tests will fail with 503 errors until the quota resets at UTC midnight. Check Azure Portal → App Service → Overview for "Quota exceeded" status.

## Troubleshooting

### Issue: "Failed to initialize document repository"

**Causes:**

1. Embeddings file doesn't exist at configured path
2. OpenAI API key not set or invalid
3. embeddings.json is corrupted or empty

**Solutions:**

```bash
# Check embeddings file exists
ls Backend.API/Data/embeddings.json

# Verify OpenAI API key is set
cd Backend.API
dotnet user-secrets list

# Set OpenAI API key if missing
dotnet user-secrets set "BackendOptions:OpenAIApiKey" "sk-your-key-here"

# Regenerate embeddings if file is corrupted
cd ../../Preprocessor/Preprocessor
dotnet run -- --provider openai
cp bin/Debug/net9.0/output.json ../../backend/Backend.API/Data/embeddings.json
```

### Issue: "Groq API key is not set" (when using Groq provider)

**Solution:**

```bash
cd Backend.API
dotnet user-secrets set "BackendOptions:GroqApiKey" "gsk-your-key-here"
dotnet user-secrets set "BackendOptions:LlmProvider" "Groq"

# Verify secrets are set
dotnet user-secrets list
```

### Issue: "Unknown LLM provider" error

**Solution:**

```bash
# Ensure LlmProvider is set to either "OpenAI" or "Groq" (case-insensitive)
cd Backend.API
dotnet user-secrets set "BackendOptions:LlmProvider" "OpenAI"
# OR
dotnet user-secrets set "BackendOptions:LlmProvider" "Groq"
```

### Issue: CORS errors from frontend

**Solution:**

Update `Program.cs` to include your frontend URL:

```csharp
policy.WithOrigins("http://localhost:3000", "https://<your-static-web-app-name>.azurestaticapps.net")
```

## Customizing the System Prompt

The system prompt instructs the LLM how to behave when answering questions. By default, it's configured for financial fund documents (see `ApplicationOptions.GetDefaultSystemPrompt()` for the default prompt).

### Customizing via Configuration

- **Be specific** about the domain (e.g., "financial documents", "medical records", "legal contracts")
- **Set boundaries** - instruct the LLM to only use provided context
- **Define tone** - professional, friendly, concise, detailed, etc.
- **Handle missing info** - specify what to say when the answer isn't in the context

## DDD Architecture Details

The backend follows **Domain-Driven Design** principles to ensure maintainability and testability:

**Domain Layer** (`Domain/`)

- **Zero external dependencies** - Pure business logic
- **Interfaces**: Define contracts (ILlmProvider, IDocumentRepository, ISemanticSearch)
- **Models**: Core entities (DocumentChunk, SearchResult, QuestionAnswer)
- **Value Objects**: Immutable, validated data (DocumentMetadata, EmbeddingVector)
- **Domain Services**: Pure computation (CosineSimilarityCalculator - deprecated, replaced by VectorStore)

**ApplicationCore Layer** (`ApplicationCore/`)

- **Use cases**: QuestionAnsweringService orchestrates the RAG pipeline
- **DTOs**: Data transfer objects for API communication
- **Depends only on Domain layer** - No infrastructure concerns

**Infrastructure Layer** (`Infrastructure/`)

- **Implements domain interfaces** with external dependencies
- **LLM Providers**: OpenAiProvider, GroqProvider (implements ILlmProvider)
- **Repository**: FileBasedDocumentRepository (implements IDocumentRepository)
- **Search**: InMemorySemanticSearch using Semantic Kernel VectorStore (implements ISemanticSearch)
- **Adapters**: SemanticKernelEmbeddingGenerator (adapts Semantic Kernel to domain interface)
- **Models**: DocumentChunkRecord (VectorStore record with SK attributes)

**Presentation Layer** (`Controllers/`)

- **Thin controllers**: Validation and delegation only
- **No business logic** - delegates to ApplicationCore services

**Dependency Flow**: `Presentation → ApplicationCore → Domain ← Infrastructure`

## Documentation

- **[Configuration & Secrets Guide](../docs/SECRETS-MANAGEMENT.md)** - All environment variables, API keys, and settings
- **[Azure Deployment Guide](../docs/AZURE-DEPLOYMENT.md)** - Complete guide for deploying to Azure App Service
- **[Project Status](../Status.md)** - Overall project progress and implementation status
